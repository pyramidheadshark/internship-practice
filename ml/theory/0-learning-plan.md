# üéì –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Å—Ç–∞–∂–∏—Ä–æ–≤–∫–µ ML-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ üå≥ (–£–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ —Ç–µ–æ—Ä–∏–∏)

**–¶–µ–ª—å:** –ü–æ–ª—É—á–∏—Ç—å —É–≥–ª—É–±–ª–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏ —Ç–≤–µ—Ä–¥–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–æ—Ä–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —É—Å–ø–µ—à–Ω–æ–π —Å—Ç–∞–∂–∏—Ä–æ–≤–∫–∏ ML-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞.

## I. **–û—Å–Ω–æ–≤—ã –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö** üíªüìä

### 1. **A. Python –¥–ª—è ML üêç:**

    #### 1.1. **NumPy üî¢:**
        *   –ú–∞—Å—Å–∏–≤—ã NumPy –∫–∞–∫ –æ—Å–Ω–æ–≤–∞ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ –º–∞—Ç—Ä–∏—Ü –≤ ML.
        *   –ë–∞–∑–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –º–∞—Å—Å–∏–≤–∞–º–∏, —Ç–∞–∫–∏–µ –∫–∞–∫ —Å–ª–æ–∂–µ–Ω–∏–µ, —É–º–Ω–æ–∂–µ–Ω–∏–µ –∏ –ø–æ—ç–ª–µ–º–µ–Ω—Ç–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏.
            *   *–í–≤–µ–¥–µ–Ω–∏–µ –≤ **–≤–µ–∫—Ç–æ—Ä—ã –∏ –º–∞—Ç—Ä–∏—Ü—ã (vectors and matrices)***
    #### 1.2. **Pandas üêº:**
        *   DataFrame –∫–∞–∫ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
        *   –†–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ —á—Ç–µ–Ω–∏—è, –∑–∞–ø–∏—Å–∏, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ –¥–∞–Ω–Ω—ã–º–∏.

### 2. **B. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (—Å Matplotlib/Seaborn) üìä:**

    #### 2.1. –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–∏–ø—ã –≥—Ä–∞—Ñ–∏–∫–æ–≤ üìà:
        *   –í–∫–ª—é—á–∞—é—Ç –ª–∏–Ω–µ–π–Ω—ã–µ, —Ç–æ—á–µ—á–Ω—ã–µ, –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –∏ –¥—Ä—É–≥–∏–µ.
    #### 2.2. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö üìâ.

## II. **–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏** üß†üìè

### 1. **A. –¢–∏–ø—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è ü§ñ:**

    #### 1.1. –û–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º (Supervised Learning) üë®‚Äçüè´:
        *   –û–±–∑–æ—Ä.
    #### 1.2. –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è (Unsupervised Learning) üßë‚Äçüè´:
        *   –û–±–∑–æ—Ä.
    #### 1.3. –û–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning) üéÆ:
        *   –û–±–∑–æ—Ä.

### 2. **B. –ü—Ä–æ—Ü–µ—Å—Å —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ ML-–º–æ–¥–µ–ª–∏ ‚öôÔ∏è:**

    #### 2.1. –û–±–∑–æ—Ä —ç—Ç–∞–ø–æ–≤ üó∫Ô∏è.

### 3. **C. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç—Ä–∏–∫–∏ üéØ:**

    #### 3.1. –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ üìè:
        *   MSE, MAE, MAPE, **RMSE, R-squared, Adjusted R-squared**.
            *   *–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ **—Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è (Mean Squared Error)** –∏ **–∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è (Mean Absolute Error)***.
    #### 3.2. –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ üìä:
        *   Accuracy, Precision, Recall, F1-score, ROC AUC, PR AUC, **Log Loss (Cross-Entropy)**.
            *   *–í–≤–µ–¥–µ–Ω–∏–µ –≤ **–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (probability)** –∏ **—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å (statistical significance)** –ø—Ä–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫*.
    #### 3.3. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (Confusion Matrix)
    #### 3.4. –í—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏ ü§î
    #### 3.5. Online/Offline –º–µ—Ç—Ä–∏–∫–∏ üîÑ
    #### 3.6. **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –º–µ—Ç—Ä–∏–∫ üî¨, –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã (confidence intervals), p-value, power analysis.**
    #### 3.7. **–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π: –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è üß™ (k-fold, stratified k-fold, leave-one-out), nested cross-validation.**
    #### 3.8. **–î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –±–æ—Ä—å–±—ã ‚öñÔ∏è (oversampling, undersampling, class weights), cost-sensitive learning.**

### 4. **D. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ üìâüìà:**

    #### 4.1. Bias-Variance Tradeoff ü§π.**
            *   *–ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ **—Å–º–µ—â–µ–Ω–∏—è (Bias)** –∏ **–¥–∏—Å–ø–µ—Ä—Å–∏–∏ (Variance)** –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–π*.
    #### 4.2. –ú–µ—Ç–æ–¥—ã –±–æ—Ä—å–±—ã —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º (–æ–±–∑–æ—Ä, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∞ –ø–æ–∑–∂–µ) üõ°Ô∏è.
    #### 4.3. Learning Curves (–∫—Ä–∏–≤—ã–µ –æ–±—É—á–µ–Ω–∏—è) –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ Bias –∏ Variance üìä.

## III. **–õ–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Linear Models)** üìà

### 1. **A. –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è:**

    #### 1.1. –ü—Ä–æ—Å—Ç–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è (–æ–¥–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è) üßç‚Äç‚ôÄÔ∏è.
    #### 1.2. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ üìê:
        *   –£—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä—è–º–æ–π üìè.
        *   **–§—É–Ω–∫—Ü–∏—è –æ—à–∏–±–∫–∏ (MSE) –∫–∞–∫ —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏ üìù.**
        *   **–ú–µ—Ç–æ–¥ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ (Least Squares) - –∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Å—É–º–º—ã –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π üìâ.**
        *   **–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ (Gradient Descent) -  –≤–≤–µ–¥–µ–Ω–∏–µ –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é, –ø–æ–Ω—è—Ç–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (—á–∞—Å—Ç–Ω—ã—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã—Ö) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø–æ–∏—Å–∫–∞ –º–∏–Ω–∏–º—É–º–∞ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—à–∏–±–∫–∏ üöÄ.**
            *   *–ó–¥–µ—Å—å –≤–≤–æ–¥–∏–º **–ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ (derivatives) –∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (gradients)***.
        *   **–†–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ (Batch GD, Stochastic GD, Mini-batch GD), momentum, adaptive learning rates (Adam, RMSprop –∏ —Ç.–¥.) üèÉ‚Äç‚ôÇÔ∏è.**
        *   **–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (normal equation) ü§ì.**
    #### 1.3. –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è (–Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö) üë®‚Äçüë©‚Äçüëß‚Äçüë¶.
    #### 1.4. **–í–µ–∫—Ç–æ—Ä–Ω–∞—è –∏ –º–∞—Ç—Ä–∏—á–Ω–∞—è –∑–∞–ø–∏—Å—å –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ üî¢.**
            *   *–£–≥–ª—É–±–ª–µ–Ω–∏–µ –≤ **–ª–∏–Ω–µ–π–Ω—É—é –∞–ª–≥–µ–±—Ä—É (linear algebra) (–≤–µ–∫—Ç–æ—Ä—ã, –º–∞—Ç—Ä–∏—Ü—ã, –º–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ)** –¥–ª—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π*.
    #### 1.5. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ (–º–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏) üìä.
    #### 1.6. **–û–±–æ–±—â–µ–Ω–Ω—ã–µ –ª–∏–Ω–µ–π–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Generalized Linear Models - GLM): –ü—É–∞—Å—Å–æ–Ω–æ–≤—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è, –ì–∞–º–º–∞-—Ä–µ–≥—Ä–µ—Å—Å–∏—è üí°.**

### 2. **B. –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –ª–∏–Ω–µ–π–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π üéõÔ∏è:**

    #### 2.1. L1 (Lasso) –∏ L2 (Ridge) —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è üîó.**
    #### 2.2. –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ üìù:
        *   –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—Ç—Ä–∞—Ñ–∞ –∫ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—à–∏–±–∫–∏  —à—Ç—Ä–∞—Ñ.
        *   –í–ª–∏—è–Ω–∏–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –Ω–∞ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∏ **–¥–∏—Å–ø–µ—Ä—Å–∏—é (Variance)** üìâ.
            *   *–°–≤—è–∑—å —Å **Bias-Variance Tradeoff***.

### 3. **C. –õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è üö™:**

    #### 3.1. –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è  –¥–≤–æ–∏—á–Ω—ã–π_–∫–æ–¥.
    #### 3.2. –°–∏–≥–º–æ–∏–¥–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è (–ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è)  sigmoid.**
            *   *–í–≤–µ–¥–µ–Ω–∏–µ –≤ **–Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (non-linear functions)** –∏ –∏—Ö —Ä–æ–ª—å –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏*.
    #### 3.3. –§—É–Ω–∫—Ü–∏—è –æ—à–∏–±–∫–∏ (Log Loss/Cross-Entropy) –¥–ª—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ üìâ.**
            *   *–û–±—ä—è—Å–Ω–µ–Ω–∏–µ, –ø–æ—á–µ–º—É MSE –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∏ –≤–≤–µ–¥–µ–Ω–∏–µ **–∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏–∏ (cross-entropy)** –∫–∞–∫ –º–µ—Ä—ã —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π*.
    #### 3.4. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ (–º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏) üìä.
    #### 3.5. –ú—É–ª—å—Ç–∏–∫–ª–∞—Å—Å–æ–≤–∞—è –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è (Softmax) üåà.

## IV. **–î—Ä–µ–≤–æ–≤–∏–¥–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Tree-based Models) –∏ –∞–Ω—Å–∞–º–±–ª–∏ (Ensembles)** üå≥üå≤

### 1. **A. –î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π üå≥:**

    #### 1.1. –ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π (—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤) üå≤.
    #### 1.2. –ê–ª–≥–æ—Ä–∏—Ç–º—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –¥–µ—Ä–µ–≤—å–µ–≤ ‚öôÔ∏è:
        *   CART, ID3, C4.5 - *–∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, **—ç–Ω—Ç—Ä–æ–ø–∏—è (entropy)** –∏ **–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç (information gain) -  –≤–≤–µ–¥–µ–Ω–∏–µ –≤ —Ç–µ–æ—Ä–∏—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏***.
    #### 1.3. **–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤ –¥–µ—Ä–µ–≤—å—è—Ö —Ä–µ—à–µ–Ω–∏–π üìù:**
        *   Gini impurity, Chi-squared test, variance reduction.
    #### 1.4. –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –¥–µ—Ä–µ–≤—å–µ–≤ –∏ –º–µ—Ç–æ–¥—ã –±–æ—Ä—å–±—ã —Å –Ω–∏–º üõ°Ô∏è:
        *   –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –≥–ª—É–±–∏–Ω—ã, –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –ª–∏—Å—Ç–µ.
    #### 1.5. **–ú–µ—Ç–æ–¥—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π üéõÔ∏è:**
        *   cost complexity pruning, reduced error pruning.
    #### 1.6. **Bias-Variance Tradeoff –≤ –¥–µ—Ä–µ–≤—å—è—Ö —Ä–µ—à–µ–Ω–∏–π ü§π.**

### 2. **B. –°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å üå≤üå≤üå≤:**

    #### 2.1. –ê–Ω—Å–∞–º–±–ª—å –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π (Bagging) üå≥üå≥.
    #### 2.2. –°–ª—É—á–∞–π–Ω—ã–π –≤—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ üé≤.
    #### 2.3. **–£–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ (Variance) –∑–∞ —Å—á–µ—Ç —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–µ—Ä–µ–≤—å–µ–≤ üìâ.**
            *   *–£–≥–ª—É–±–ª–µ–Ω–∏–µ –≤ –ø–æ–Ω–∏–º–∞–Ω–∏–µ **–¥–∏—Å–ø–µ—Ä—Å–∏–∏ (Variance)** –∏ –µ–µ —Ä–æ–ª–∏ –≤ –∞–Ω—Å–∞–º–±–ª—è—Ö*.
    #### 2.4. Feature Importance –≤ Random Forest üåü.

### 3. **C. –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ üöÄ:**

    #### 3.1. –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π (Boosting) ‚û°Ô∏è.
    #### 3.2. **–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ñ—É–Ω–∫—Ü–∏–π üåå.**
            *   *–ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ **–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ (Gradient Descent)** –∏ –µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –Ω–µ —Ç–æ–ª—å–∫–æ –∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º, –Ω–æ –∏ –∫ —Ñ—É–Ω–∫—Ü–∏—è–º*.
    #### 3.3. **Loss functions –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º –±—É—Å—Ç–∏–Ω–≥–µ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á üìâ:**
        *   Log Loss, Huber loss, Quantile loss.
    #### 3.4. **Tree boosting vs. gradient boosting ü§î.**
    #### 3.5. **XGBoost, LightGBM, CatBoost üêÜ, üí°, üêà:**
            *   –†–∞–∑–ª–∏—á–∏—è –≤ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è—Ö.
            *   –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º –±—É—Å—Ç–∏–Ω–≥–µ.
    #### 3.6. Feature Importance –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–º –±—É—Å—Ç–∏–Ω–≥–µ üåü.

## V. **–ú–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (SVM)** üöÄ

### 1. **A. –õ–∏–Ω–µ–π–Ω–æ–µ SVM  ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ:**

    #### 1.1. –†–∞–∑–¥–µ–ª—è—é—â–∞—è –≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—å, –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∑–∞–∑–æ—Ä (margin)  –ø–ª–æ—Å–∫–æ—Å—Ç—å.**
            *   *–ì–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è SVM, –≤–≤–µ–¥–µ–Ω–∏–µ –≤ **–≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç–∏ (hyperplanes)** –∏ **–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ (constrained optimization)***.
    #### 1.2. **–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ SVM: —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ SVM (primal –∏ dual problems), –º–µ—Ç–æ–¥ —Ä–µ—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ SVM (–Ω–∞–ø—Ä–∏–º–µ—Ä, Sequential Minimal Optimization - SMO) üìù.**
    #### 1.3. –§—É–Ω–∫—Ü–∏—è –æ—à–∏–±–∫–∏ (Hinge Loss) üìâ.**

### 2. **B. –ù–µ–ª–∏–Ω–µ–π–Ω–æ–µ SVM –∏ —è–¥—Ä–∞ (Kernels)  —è–¥—Ä–∞:**

    #### 2.1. –Ø–¥–µ—Ä–Ω–∞—è —Ç—Ä—é–∫ (Kernel trick) - *–∏–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –∫–∞–∫ —è–¥—Ä–∞ –ø–æ–∑–≤–æ–ª—è—é—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö –≤—ã—Å–æ–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –±–µ–∑ —è–≤–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç* ü™Ñ.
    #### 2.2. **–¢–µ–æ—Ä–∏—è —è–¥–µ—Ä: —Ç–µ–æ—Ä–µ–º–∞ –ú–µ—Ä—Å–µ—Ä–∞, —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã —è–¥–µ—Ä –∏ –∏—Ö —Å–≤–æ–π—Å—Ç–≤–∞, –≤—ã–±–æ—Ä —è–¥—Ä–∞ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∑–∞–¥–∞—á–∏ üìö.**
    #### 2.3. –Ø–¥—Ä–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (RBF, Polynomial, Linear)  —è–¥—Ä–∞.
    #### 2.4. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã C –∏ –≥–∞–º–º–∞ üéõÔ∏è.

## VI. **–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ (Neural Networks) - –í–≤–µ–¥–µ–Ω–∏–µ** üß†üï∏

### 1. **A. –ü–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω –∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω (MLP) üß†.**

### 2. **B. –§—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: ReLU, Sigmoid, Tanh üî•.**
        *   *–†–æ–ª—å **–Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–µ–π (non-linearities)** –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö*.

### 3. **C. –û–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ (Backpropagation) üîÑ:**
        *   *–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ **—Ü–µ–ø–Ω–æ–≥–æ –ø—Ä–∞–≤–∏–ª–∞ (chain rule)** –∏ **–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞ (Gradient Descent)** –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã—Ö —Å–µ—Ç–µ–π*.

    #### 3.1. **–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏, chain rule –≤ –¥–µ—Ç–∞–ª—è—Ö üìù.**
    #### 3.2. **Vanishing/exploding gradients problems –∏ —Å–ø–æ—Å–æ–±—ã –∏—Ö —Ä–µ—à–µ–Ω–∏—è üí•.**

### 4. **D. –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã: Adam, SGD üöÄ.**
        *   *–†–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã **–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (optimization)** –∏ –∏—Ö –≤–ª–∏—è–Ω–∏–µ –Ω–∞ —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è*.

### 5. **E. BatchNorm –∏ Dropout üõ°Ô∏è:**
        *   *–¢–µ—Ö–Ω–∏–∫–∏ **—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (regularization)** –∏ **—É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è (training acceleration)** –≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç—è—Ö*.

### 6. **F. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π (–æ–±–∑–æ—Ä–Ω–æ) üèóÔ∏è: Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM).**

### 7. **G. –ú–µ—Ç–æ–¥—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤: Xavier/Glorot initialization, He initialization ‚öñÔ∏è.**

## VII. **–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –∏ –±–æ–ª—å—à–∏–µ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ (Transformers & LLMs) - –í–≤–µ–¥–µ–Ω–∏–µ** üó£

### 1. **A. –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è (Attention Mechanism) üëÅÔ∏è:**

    #### 1.1. **–î–µ—Ç–∞–ª—å–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ –º–µ—Ö–∞–Ω–∏–∑–º–∞ self-attention: multi-head attention, scaled dot-product attention, positional encoding üìù.**

### 2. **B. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Transformer üèóÔ∏è.**

### 3. **C. BERT –∏ LLM (–æ–±–∑–æ—Ä) üìö.**

### 4. **D. Transfer learning –∏ fine-tuning –¥–ª—è LLM üîÑ.**

### 5. **E. –†–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã Transformer-based –º–æ–¥–µ–ª–µ–π: GPT, T5, BART –∏ –∏—Ö –æ—Ç–ª–∏—á–∏—è ü§î.**

## VIII. **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ–º—ã (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)** üåü (–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)

### 1. **A. –ü–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ üìâ:**

    #### 1.1. PCA, t-SNE, Umap, **LDA, NMF, Autoencoders**.
        *   *–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ **PCA (—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã, —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü)** –ø—Ä–∏ –∏–∑—É—á–µ–Ω–∏–∏ PCA*.

### 2. **B. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è üèòÔ∏è:**

    #### 2.1. K-Means, **DBSCAN, OPTICS, Mean Shift, Spectral Clustering, –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è**.
        *   **–ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (silhouette score, Davies-Bouldin index) üìä.**

### 3. **C. Feature Selection –∏ Feature Extraction ‚öôÔ∏è:**
        *   –ú–µ—Ç–æ–¥—ã –æ—Ç–±–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ñ–∏–ª—å—Ç—Ä—ã, –æ–±–µ—Ä—Ç–∫–∏, –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã), –º–µ—Ç–æ–¥—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (PCA, NMF, Autoencoders).

### 4. **D. NLP –∏ Computer Vision (–±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏) üó£Ô∏è, üëÅÔ∏è.**

### 5. **E. –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π üöÄ.**

### 6. **F. –≠—Ç–∏–∫–∞ –∏ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –≤ ML ‚öñÔ∏è.**

## –ü—Ä–∏–º–µ—á–∞–Ω–∏—è:

*   **–£–≥–ª—É–±–ª–µ–Ω–Ω–æ–µ –∏–∑—É—á–µ–Ω–∏–µ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ üìù:**  –í –∫–∞–∂–¥–æ–º —Ä–∞–∑–¥–µ–ª–µ –¥–µ–ª–∞–µ—Ç—Å—è –∞–∫—Ü–µ–Ω—Ç –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –º–µ—Ç–æ–¥–æ–≤ –∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤.
*   **–¢–≤–µ—Ä–¥–æ–µ –∑–Ω–∞–Ω–∏–µ —Ç–µ–æ—Ä–∏–∏ üß†:**  –¶–µ–ª—å - –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ, –Ω–æ –∏ –≥–ª—É–±–æ–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–Ω–æ–≤ ML.
*   **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è üßë‚Äçüíª:**  –†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞–Ω–∏–π, –≤–∫–ª—é—á–∞—é—â–∏—Ö —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ "—Å –Ω—É–ª—è".
*   **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã üìö:**  –í–∫–ª—é—á–∏—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏ –¥—Ä—É–≥–∏–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–≥–æ —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–≥–æ –∏–∑—É—á–µ–Ω–∏—è.
*   **–í–æ–ø—Ä–æ—Å—ã –¥–ª—è —Å–∞–º–æ–ø—Ä–æ–≤–µ—Ä–∫–∏ ü§î:**  –î–æ–±–∞–≤–∏—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –∑–∞–¥–∞—á–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –¥–ª—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–∞.
