# 🌳 Древовидные модели и ансамбли: мощь и гибкость в машинном обучении

Древовидные модели и их ансамбли — это один из самых популярных и мощных классов алгоритмов машинного обучения. Они широко применяются в задачах классификации и регрессии благодаря своей гибкости, интерпретируемости и способности моделировать сложные зависимости. В отличие от линейных моделей, деревья решений и их ансамбли могут улавливать нелинейные взаимосвязи и взаимодействия между признаками, что делает их универсальными инструментами в ML.

---

## 1. 🌳 Деревья решений: базовый строительный блок

Деревья решений — это алгоритмы, которые разбивают пространство признаков на области с помощью последовательных вопросов (разделений), чтобы предсказать целевую переменную. Они напоминают древо вопросов в игре "20 вопросов": каждый вопрос сужает круг возможных ответов.

### 1.1. 📍 Принцип работы: разделение пространства признаков

Дерево решений работает, рекурсивно разбивая данные на подмножества на основе значений признаков. На каждом шаге выбирается признак и порог, которые лучше всего разделяют данные по целевой переменной. Процесс продолжается, пока не достигнут критерий остановки (например, максимальная глубина или минимальное число объектов в узле).

- **Геометрическая интерпретация**: Представь пространство признаков как лист бумаги. Дерево делает "разрезы" (разделения), деля его на прямоугольники (в 2D) или гиперкубы (в многомерном случае). Каждый разрез определяется условием, например, "x₁ ≤ 5". Нарисуй два признака и разбиение — станет понятнее!
- **Пример**: Для предсказания, купит ли человек дом (да/нет), дерево может спросить: "Доход > 50K?" → "Возраст < 40?" → итоговое предсказание.

**✍️ Интуитивная аналогия**: Дерево — как повар, который сортирует фрукты. "Яблоко красное?" → "Размер больше ладони?" → "Это сорт А". Каждый вопрос упрощает задачу.

### 1.2. ⚙️ Алгоритмы построения: CART, ID3, C4.5

Существует несколько алгоритмов для построения деревьев. Они различаются по критериям разделения и типу задач:

- **CART (Classification and Regression Trees)**: Используется в sklearn. Подходит для классификации (Gini impurity) и регрессии (variance reduction).
- **ID3**: Использует энтропию и информационный прирост. Работает только с категориальными признаками.
- **C4.5**: Улучшенная версия ID3, поддерживает непрерывные признаки и пропуски.

**✍️ Аналогия**: Выбор "лучшего вопроса" в игре "20 вопросов". Информационный прирост — это мера, насколько вопрос сокращает неопределенность.

### 1.3. 📝 Математика критериев разделения

Дерево выбирает разделение, минимизируя неопределенность (для классификации) или дисперсию (для регрессии). Основные метрики:

- **Gini Impurity** (для классификации):  
  $$Gini = 1 - \sum_{k=1}^{K} p_k^2$$, где $$p_k$$ — доля объектов класса $$k$$ в узле.  
  - Если все объекты одного класса ($$p_1 = 1$$), $$Gini = 0$$ (чистота).  
  - Если классы равномерно распределены (например, 50/50), $$Gini = 0.5$$ (максимальная неопределенность).
- **Энтропия** (ID3, C4.5):  
  $$H = -\sum_{k=1}^{K} p_k \log_2(p_k)$$. Похожа на Gini, но более "гладкая".
- **Variance Reduction** (для регрессии):  
  $$Var = \frac{1}{n} \sum_{i=1}^{n} (y_i - \bar{y})^2$$. Разделение уменьшает дисперсию в дочерних узлах.

**Формула прироста качества**:  
$$Gain = Q_{parent} - \frac{n_{left}}{n} Q_{left} - \frac{n_{right}}{n} Q_{right}$$, где $$Q$$ — критерий (Gini, энтропия, дисперсия).

**✍️ Интуитивно**: Gini и энтропия — как мера "смешанности" в коробке конфет. Чем меньше видов конфет после разделения, тем лучше.

### 1.4. 🛡️ Переобучение и методы борьбы

Деревья склонны к переобучению: глубокое дерево может запомнить данные, включая шум.  
- **Признаки переобучения**: Отлично работает на обучении, плохо на тесте.
- **Методы борьбы**:  
  - **Ограничение глубины** (max_depth): Не даём дереву расти слишком глубоко.  
  - **Минимальное число объектов в узле** (min_samples_split, min_samples_leaf): Останавливаем разделение, если данных мало.  
  - **Максимальное число листьев** (max_leaf_nodes).

### 1.5. 🎛️ Регуляризация деревьев

- **Cost Complexity Pruning (CCP)**: Добавляет штраф за сложность дерева:  
  $$Loss_{pruned} = Loss + \alpha \cdot T$$, где $$T$$ — число листьев, $$\alpha$$ — гиперпараметр. Обрезаем ветви, если улучшение незначительно.  
- **Reduced Error Pruning**: Удаляем узлы, если это не ухудшает точность на валидации.

**✍️ Аналогия**: Обрезка дерева в саду — убираем лишние ветки, чтобы оно лучше росло.

### 1.6. 🤹 Bias-Variance Tradeoff

- **Неглубокое дерево**: Высокое смещение (bias), низкая дисперсия (variance). Простая модель, недообучается.  
- **Глубокое дерево**: Низкое смещение, высокая дисперсия. Сложная модель, переобучается.  
- **Задача**: Найти баланс через регуляризацию.

```
from sklearn.tree import DecisionTreeClassifier
X = [[0, 0], [1, 1], [0, 1]]  # Признаки
y = [0, 1, 1]  # Классы
model = DecisionTreeClassifier(max_depth=2)
model.fit(X, y)
print(model.predict([[0.5, 0.5]]))  # Предсказание
```

---

## 2. 🌲🌲🌲 Случайный лес: сила ансамбля

Случайный лес (Random Forest) — это ансамбль деревьев, который объединяет их предсказания для повышения точности и устойчивости.

### 2.1. 📦 Ансамбль деревьев: Bagging

- **Bagging (Bootstrap Aggregating)**:  
  1. Создаём много подвыборок из данных с возвращением (bootstrap).  
  2. Обучаем дерево на каждой подвыборке.  
  3. Усредняем предсказания (регрессия) или голосуем (классификация).  
  Bootstrap — это выборка с возвращением. Например, из 5 объектов [A, B, C, D, E] можем взять [A, A, C, E, B] — некоторые повторяются, другие пропадают. Это создаёт разнообразие между деревьями.
- **Почему работает**: Уменьшает дисперсию, так как ошибки отдельных деревьев "гасятся" при усреднении.

**✍️ Аналогия**: Спрашиваем мнение у группы друзей вместо одного. Средний ответ обычно точнее.

### 2.2. 🎲 Случайный выбор признаков

- При каждом разделении дерево выбирает не из всех признаков, а из случайного подмножества (параметр `max_features`).  
- Это добавляет разнообразие между деревьями, снижая их корреляцию.

### 2.3. 📉 Уменьшение дисперсии

- Отдельное дерево: Высокая дисперсия (чувствительно к данным).  
- Случайный лес: Низкая дисперсия за счёт усреднения. Bias почти не увеличивается, если деревья глубокие.

### 2.4. 🌟 Feature Importance

- Случайный лес оценивает важность признаков по тому, как сильно они уменьшают критерий (Gini, дисперсию) в среднем по всем деревьям.  
- **Пример**: Признак "доход" важнее "возраста", если он чаще улучшает разделение.

### 2.5. ⚖️ Сравнение с линейными моделями

- **Плюсы леса**: Ловит нелинейности, не требует нормализации данных.  
- **Минусы**: Менее интерпретируем, медленнее обучается.  
- **Линейные модели**: Быстрее, проще, но ограничены линейными зависимостями. Нейронные сети же выигрывают на больших данных, например, изображениях.

```
from sklearn.ensemble import RandomForestClassifier
X = [[0, 0], [1, 1], [0, 1]]
y = [0, 1, 1]
model = RandomForestClassifier(n_estimators=10, max_depth=2)
model.fit(X, y)
print(model.feature_importances_)  # Важность признаков
```

---

## 3. 🚀 Градиентный бустинг: последовательная мощь

Градиентный бустинг — это метод, где деревья обучаются последовательно, каждое исправляя ошибки предыдущих.

### 3.1. 📈 Последовательное построение: Boosting

- **Идея**: Первое дерево предсказывает плохо → второе корректирует его ошибки → третье улучшает ещё больше.  
- **Отличие от Bagging**: Bagging независим, Boosting — последовательный.  
  Аналогия: Как художники рисуют картину. Первый делает грубый набросок, второй добавляет детали, третий корректирует ошибки — каждый улучшает работу предыдущего.

**✍️ Аналогия**: Команда, где каждый участник исправляет ошибки предыдущего.

### 3.2. 🌌 Градиентный спуск в пространстве функций

- **Цель**: Минимизировать функцию потерь $$L(y, \hat{y})$$ (например, MSE, Log Loss).  
- **Как**: Каждое дерево добавляет шаг в направлении антиградиента ошибки.  
- **Математика**:  
  $$\hat{y}_{new} = \hat{y}_{old} + \eta \cdot h(x)$$, где $$h(x)$$ — предсказание нового дерева, $$\eta$$ — learning rate.  
  Пример: Допустим, у нас регрессия, и истинные значения [3, 5]. Первое дерево предсказывает среднее (4, 4). Ошибка: [-1, 1]. Второе дерево учится предсказывать эту ошибку, добавляя корректировку. И так далее.

### 3.3. 📉 Функции потерь

- **Log Loss**: Для классификации, $$-\sum [y \log(\hat{p}) + (1-y) \log(1-\hat{p})]$$.  
- **Huber Loss**: Комбинация MSE и MAE, устойчива к выбросам.  
- **Quantile Loss**: Для предсказания квантилей (например, 95%).

### 3.4. 🌳 Tree Boosting vs Gradient Boosting

- **Tree Boosting**: Использование деревьев как базовых моделей в градиентном бустинге.  
- **Общий Gradient Boosting**: Может использовать любые функции (не только деревья).

### 3.5. 🐆 XGBoost, LightGBM, CatBoost

- **XGBoost**:  
  - Оптимизация: Быстрее благодаря гистограммному подходу.  
  - Регуляризация: L1/L2 на листьях.  
- **LightGBM**:  
  - Оптимизация: Leaf-wise рост (быстрее для больших данных).  
  - Поддержка GPU.  
- **CatBoost**:  
  - Категориальные признаки: Автоматическая обработка без One-Hot Encoding.  
  - Устойчивость к переобучению.

**✍️ Аналогия**: Три спортсмена. XGBoost — универсал, LightGBM — быстрый бегун, CatBoost — мастер точных ударов.

### 3.6. 🌟 Feature Importance

- Оценивается по вкладу признаков в уменьшение ошибки. Может отличаться от Random Forest (фокус на последовательной коррекции).

### 3.7. 💡 AdaBoost: Альтернатива бустингу

- **Идея**: Взвешивает объекты, увеличивая вес ошибочных.  
- **Отличие**: Простые деревья (stumps), экспоненциальная потеря.

```
from sklearn.ensemble import GradientBoostingClassifier
X = [[0, 0], [1, 1], [0, 1]]
y = [0, 1, 1]
model = GradientBoostingClassifier(n_estimators=10, learning_rate=0.1)
model.fit(X, y)
print(model.predict([[0.5, 0.5]]))
```

---

## 🎯 Ключевые выводы

- **Деревья решений**: Просты, интерпретируемы, но склонны к переобучению.  
- **Случайный лес**: Устойчив, точен, снижает дисперсию.  
- **Градиентный бустинг**: Мощный, адаптивный, требует настройки.  

**Практическое задание**:  
1. Построй дерево решений на своих данных. Какой критерий разделения выбрал бы ты и почему?  
2. Сравни Random Forest и Gradient Boosting на задаче классификации. Что влияет на результат?  
3. Обучи Random Forest с разным числом деревьев (10, 50, 100). Как меняется точность и время обучения? Почему? Попробуй измерить с помощью `time.time()`.